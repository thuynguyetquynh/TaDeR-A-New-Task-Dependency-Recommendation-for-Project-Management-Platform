{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Get_GloVe_features.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pbSuD_3JgzAk"},"source":["# Library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRiQVMhngzAm","executionInfo":{"status":"ok","timestamp":1634440675550,"user_tz":-420,"elapsed":556,"user":{"displayName":"Khoang Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05898184266481346839"}},"outputId":"630a4f3b-035b-44a2-94f0-350eab71706a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"oA6TR9ongzAm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634440684739,"user_tz":-420,"elapsed":8695,"user":{"displayName":"Khoang Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05898184266481346839"}},"outputId":"e44c6122-c6e6-4bae-95ef-215852d5ce06"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import networkx as nx\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import datetime\n","import gc\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk import word_tokenize\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","from keras.preprocessing.sequence import pad_sequences\n","\n","tf.random.set_seed(123)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_pxWxxCcCJw","executionInfo":{"status":"ok","timestamp":1634440684741,"user_tz":-420,"elapsed":27,"user":{"displayName":"Khoang Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05898184266481346839"}},"outputId":"ccfec74f-2349-40bd-c621-f33b830e6dca"},"source":["cd /content/drive/MyDrive/AISIA/Jira recommendation/"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AISIA/Jira recommendation\n"]}]},{"cell_type":"markdown","metadata":{"id":"MTlk8R4WfvUu"},"source":["# Load dataset"]},{"cell_type":"code","metadata":{"id":"WUAyPH7y26Cc","executionInfo":{"status":"ok","timestamp":1634440684742,"user_tz":-420,"elapsed":22,"user":{"displayName":"Khoang Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05898184266481346839"}}},"source":["def encode_graph(row):\n","  new_row = []\n","  for i in row:\n","    if i==0:\n","      new_row.append([1,0])\n","    else:\n","      new_row.append([0,1])\n","  return new_row"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0vt5UFbuSQ1","executionInfo":{"status":"ok","timestamp":1634440684743,"user_tz":-420,"elapsed":22,"user":{"displayName":"Khoang Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05898184266481346839"}}},"source":["def load_project(project_name):\n","  # Attributes\n","  df = pd.read_csv('data/{}/attribute_preprocess.csv'.format(project_name))\n","  df = df.fillna('')\n","  # Graph\n","  graph = pd.read_csv('data/{}/graph.csv'.format(project_name), delimiter=',')\n","  graph = graph.apply(encode_graph)\n","  graph = graph.values\n","  return df, graph"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9k8yA8QMujWw"},"source":["## GloVe"]},{"cell_type":"code","metadata":{"id":"voYo6gqlaSv2"},"source":["def load_word_embeddings(fname):\n","    wordvecs = {}\n","    with open(fname, 'r') as file:\n","        lines = file.readlines()\n","        for line in lines:\n","            tokens = line.split(' ')\n","            vec = np.array(tokens[1:], dtype=np.float32)\n","            wordvecs[tokens[0]] = vec\n","\n","    return wordvecs \n","\n","wordvecs = load_word_embeddings(\"embedding/glove.42B.300d.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_94AEvOMR8I"},"source":["__PADDED_INDEX__ = 0 \n","__UNKNOWN_WORD__ = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMsrV32sPFaa"},"source":["vocab = wordvecs.keys()\n","matrix = list(wordvecs.values())\n","del wordvecs\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLc7nY0UYp6D"},"source":["word_to_index = {word: index+2 for index, word in enumerate(vocab)}\n","del vocab\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0V2cXjDjS8_O"},"source":["embedding_matrix = np.pad(matrix, [[2,0],[0,0]], mode='constant', constant_values =0.0)\n","del matrix\n","gc.collect()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qXTofqjTfxIo"},"source":["len(word_to_index), len(embedding_matrix)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8IhkpwRHob0R"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"Ly-enb4GP03f"},"source":["### Get features"]},{"cell_type":"code","metadata":{"id":"HA78gq_IhZah"},"source":["def glove_tokenizer(sentences):\n","  tokenized_texts = [nltk.tokenize.word_tokenize(text) for text in sentences]\n","  X = []\n","  for text in tokenized_texts:\n","    cur_text_indices = []\n","    for word in text:\n","      if word in word_to_index:\n","          cur_text_indices.append(word_to_index[word])    \n","      else:\n","          cur_text_indices.append(__UNKNOWN_WORD__)  \n","    X.append(cur_text_indices)\n","  return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0uKrgeS9HkpU"},"source":["def get_textual_features(df):\n","  df[\"title\"] = df[\"title\"].str.replace(\"[ ]+\", \" \", regex=True).str.strip()\n","  df[\"description\"] = df[\"description\"].str.replace(\"[ ]+\", \" \", regex=True).str.strip()\n","  df[\"summary\"] = df[\"summary\"].str.replace(\"[ ]+\", \" \", regex=True).str.strip()\n","\n","  # Extract data from dataframe\n","  titles = df['title'].values\n","  descriptions = df['description'].values\n","  summaries = df['summary'].values\n","\n","  return titles, descriptions, summaries"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9J_JCdZ1hFll"},"source":["def return_model(value_maxlen):\n","  inputs_A = keras.Input(shape=(value_maxlen), name=\"input_a\")\n","\n","  embedding_layer = keras.layers.Embedding(input_dim=embedding_matrix.shape[0],   \n","                 output_dim=embedding_matrix.shape[1],   \n","                  embeddings_initializer = tf.keras.initializers.Constant(value=embedding_matrix),  \n","                  trainable=False,                     \n","                 mask_zero=True)                 \n","\n","  # Embedding\n","  emb_A = embedding_layer(inputs_A)\n","  \n","  model = keras.Model(inputs=[inputs_A], outputs=emb_A)\n","  model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"categorical_accuracy\"])\n","  \n","  model.summary()\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02oiSc4QQwlA"},"source":["### Start"]},{"cell_type":"code","metadata":{"id":"nxT9qdSmQhbi"},"source":["list_project_names = [('FLUME', 1577, 5, 200, 256), ('MDLSITE', 4100, 12, 200, 256)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kmt0pJTpoaqS"},"source":["for project in list_project_names:\n","  project_name = project[0]\n","  time_split = project[1]\n","\n","  # Model params\n","  steps_per_epoch = project[2]\n","  epochs = project[3]\n","  batch_size = project[4]\n","\n","  # Load dataset\n","  df, graph = load_project(project_name)\n","\n","  # Get features\n","  titles, descriptions, summaries = get_textual_features(df)\n","  del df\n","  del graph\n","  gc.collect()\n","\n","  # Save path\n","  path = 'embedding/glove/{}/'.format(project_name)\n","  try:\n","    os.mkdir(path)\n","  except:\n","    print('Cannot create path {}'.format(path))\n","\n","  # All textual features\n","  value_maxlen = 540\n","  all_text = [descriptions[i] +' '+titles[i] + ' '+summaries[i] for i in range(0, len(titles))]\n","  model = return_model(value_maxlen)\n","  \n","  save_path = path\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  tokenized = glove_tokenizer(all_text)\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  feature = model.predict(padded)\n","  np.save(save_path + 'textual_features.npy', feature) \n","  del all_text\n","  del tokenized\n","  del model\n","  gc.collect()\n","\n","  value_maxlen = 20\n","  # Only title\n","  model = return_model(value_maxlen)\n","  save_path = path + \"title/\"\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  tokenized = glove_tokenizer(titles)\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  del tokenized\n","  gc.collect()\n","\n","  feature = model.predict(padded)\n","  del padded\n","  gc.collect()\n","\n","  np.save(save_path + 'textual_features.npy', feature) \n","  del model\n","  gc.collect()\n","\n","  # Only summary\n","  model = return_model(value_maxlen)\n","  save_path = path + \"summary/\"\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  tokenized = glove_tokenizer(summaries)\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  del tokenized\n","  gc.collect()\n","\n","  feature = model.predict(padded)\n","  del padded\n","  gc.collect()\n","  \n","  np.save(save_path + 'textual_features.npy', feature) \n","  del model\n","  gc.collect()\n","\n","  value_maxlen = 500\n","  # Only description\n","  model = return_model(value_maxlen)\n","  save_path = path + \"description/\"\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  tokenized = glove_tokenizer(descriptions)\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  del tokenized\n","  gc.collect()\n","\n","  feature = model.predict(padded)\n","  del padded\n","  gc.collect()\n","\n","  np.save(save_path + 'textual_features.npy', feature) \n","  del model\n","  gc.collect()\n","\n","  value_maxlen = 520\n","  # description + title\n","  model = return_model(value_maxlen)\n","  save_path = path + \"description_title/\"\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  all_text = [descriptions[i] +' '+titles[i] for i in range(0, len(titles))]\n","  tokenized = glove_tokenizer(all_text)\n","\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  del tokenized\n","  gc.collect()\n","\n","  feature = model.predict(padded)\n","  del padded\n","  gc.collect()\n","  \n","  np.save(save_path + 'textual_features.npy', feature) \n","  del all_text\n","  del model\n","  gc.collect()\n","\n","  # description + summary\n","  model = return_model(value_maxlen)\n","  save_path = path + \"description_summary/\"\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  all_text = [descriptions[i] +' '+summaries[i] for i in range(0, len(titles))]\n","  tokenized = glove_tokenizer(all_text)\n","\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  del tokenized\n","  gc.collect()\n","\n","  feature = model.predict(padded)\n","  del padded\n","  gc.collect()\n","  \n","  np.save(save_path + 'textual_features.npy', feature) \n","  del all_text\n","  del model\n","  gc.collect()\n","\n","  # title + summary\n","  model = return_model(value_maxlen)\n","  valye_maxlen = 40\n","  save_path = path + \"title_summary/\"\n","  try:\n","    os.mkdir(save_path)\n","  except:\n","    print('Cannot create path {}'.format(save_path))\n","  all_text = [titles[i] +' '+summaries[i] for i in range(0, len(titles))]\n","  tokenized = glove_tokenizer(all_text)\n","  padded = pad_sequences(tokenized, maxlen=value_maxlen, padding = 'post', truncating=\"post\")\n","  del tokenized\n","  gc.collect()\n","\n","  feature = model.predict(padded)\n","  del padded\n","  gc.collect()\n","  \n","  np.save(save_path + 'textual_features.npy', feature) \n","  del all_text\n","  del model\n","  gc.collect()"],"execution_count":null,"outputs":[]}]}